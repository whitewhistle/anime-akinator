{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/whitewhistle22116026/webscraping?scriptVersionId=122593081\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","execution_count":1,"id":"03453ab7","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-03-18T19:20:40.192455Z","iopub.status.busy":"2023-03-18T19:20:40.19179Z","iopub.status.idle":"2023-03-18T19:20:40.202197Z","shell.execute_reply":"2023-03-18T19:20:40.2013Z"},"papermill":{"duration":0.02141,"end_time":"2023-03-18T19:20:40.204756","exception":false,"start_time":"2023-03-18T19:20:40.183346","status":"completed"},"tags":[]},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n"]},{"cell_type":"code","execution_count":2,"id":"785c64a8","metadata":{"execution":{"iopub.execute_input":"2023-03-18T19:20:40.21776Z","iopub.status.busy":"2023-03-18T19:20:40.217277Z","iopub.status.idle":"2023-03-18T19:21:16.807848Z","shell.execute_reply":"2023-03-18T19:21:16.806117Z"},"papermill":{"duration":36.600665,"end_time":"2023-03-18T19:21:16.811005","exception":false,"start_time":"2023-03-18T19:20:40.21034","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (2.28.2)\r\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests) (1.26.14)\r\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests) (2022.12.7)\r\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests) (2.1.1)\r\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests) (3.4)\r\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n","\u001b[0mCollecting bs4\r\n","  Downloading bs4-0.0.1.tar.gz (1.1 kB)\r\n","  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n","\u001b[?25hRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from bs4) (4.11.1)\r\n","Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\r\n","Building wheels for collected packages: bs4\r\n","  Building wheel for bs4 (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n","\u001b[?25h  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1271 sha256=cf70132c672cbe657dc147c29be561c85aafdeb1c85697b711533c2e63bed9c8\r\n","  Stored in directory: /root/.cache/pip/wheels/77/8a/04/7b1a8ce5de6555a18e09370d3d4fde48be9571ac07a623071e\r\n","Successfully built bs4\r\n","Installing collected packages: bs4\r\n","Successfully installed bs4-0.0.1\r\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n","\u001b[0mRequirement already satisfied: html5lib in /opt/conda/lib/python3.7/site-packages (1.1)\r\n","Requirement already satisfied: six>=1.9 in /opt/conda/lib/python3.7/site-packages (from html5lib) (1.16.0)\r\n","Requirement already satisfied: webencodings in /opt/conda/lib/python3.7/site-packages (from html5lib) (0.5.1)\r\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n","\u001b[0m"]}],"source":["!pip install requests\n","!pip install bs4\n","!pip install html5lib"]},{"cell_type":"code","execution_count":null,"id":"2f02cafb","metadata":{"papermill":{"duration":0.006368,"end_time":"2023-03-18T19:21:16.823721","exception":false,"start_time":"2023-03-18T19:21:16.817353","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"raw","id":"463aef41","metadata":{"papermill":{"duration":0.005873,"end_time":"2023-03-18T19:21:16.835761","exception":false,"start_time":"2023-03-18T19:21:16.829888","status":"completed"},"tags":[]},"source":[]},{"cell_type":"code","execution_count":null,"id":"64e56100","metadata":{"papermill":{"duration":0.005849,"end_time":"2023-03-18T19:21:16.847837","exception":false,"start_time":"2023-03-18T19:21:16.841988","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"72089ea2","metadata":{"papermill":{"duration":0.005848,"end_time":"2023-03-18T19:21:16.859793","exception":false,"start_time":"2023-03-18T19:21:16.853945","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"86a06d40","metadata":{"papermill":{"duration":0.005749,"end_time":"2023-03-18T19:21:16.871607","exception":false,"start_time":"2023-03-18T19:21:16.865858","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"4e6eac55","metadata":{"papermill":{"duration":0.005714,"end_time":"2023-03-18T19:21:16.883495","exception":false,"start_time":"2023-03-18T19:21:16.877781","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"ba217478","metadata":{"papermill":{"duration":0.006097,"end_time":"2023-03-18T19:21:16.895633","exception":false,"start_time":"2023-03-18T19:21:16.889536","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"569a3efb","metadata":{"papermill":{"duration":0.005657,"end_time":"2023-03-18T19:21:16.907202","exception":false,"start_time":"2023-03-18T19:21:16.901545","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"a197987a","metadata":{"papermill":{"duration":0.006027,"end_time":"2023-03-18T19:21:16.920211","exception":false,"start_time":"2023-03-18T19:21:16.914184","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"5bdf69e7","metadata":{"papermill":{"duration":0.005636,"end_time":"2023-03-18T19:21:16.931756","exception":false,"start_time":"2023-03-18T19:21:16.92612","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"markdown","id":"947f8697","metadata":{"papermill":{"duration":0.005833,"end_time":"2023-03-18T19:21:16.94414","exception":false,"start_time":"2023-03-18T19:21:16.938307","status":"completed"},"tags":[]},"source":[]},{"cell_type":"code","execution_count":null,"id":"6b267f3c","metadata":{"papermill":{"duration":0.005711,"end_time":"2023-03-18T19:21:16.956327","exception":false,"start_time":"2023-03-18T19:21:16.950616","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"295c9c01","metadata":{"papermill":{"duration":0.005696,"end_time":"2023-03-18T19:21:16.968005","exception":false,"start_time":"2023-03-18T19:21:16.962309","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"3b7d86da","metadata":{"papermill":{"duration":0.005572,"end_time":"2023-03-18T19:21:16.979502","exception":false,"start_time":"2023-03-18T19:21:16.97393","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"107399de","metadata":{"papermill":{"duration":0.005727,"end_time":"2023-03-18T19:21:16.991006","exception":false,"start_time":"2023-03-18T19:21:16.985279","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":3,"id":"e24ec887","metadata":{"execution":{"iopub.execute_input":"2023-03-18T19:21:17.005281Z","iopub.status.busy":"2023-03-18T19:21:17.004872Z","iopub.status.idle":"2023-03-18T19:21:17.761115Z","shell.execute_reply":"2023-03-18T19:21:17.759495Z"},"papermill":{"duration":0.7672,"end_time":"2023-03-18T19:21:17.764077","exception":false,"start_time":"2023-03-18T19:21:16.996877","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["2-D Lovers\n","A real life girlfriend or boyfriend? No way! These characters would much rather be dating their favorite 2-D character from an anime, manga, visual novel or other form of media.\n","Abnormally Tall\n","Abnormally Tall characters are substantially taller than other members of the cast, often towering over their cohorts. This unique attribute is often a source of ridicule from their peers.\n","Actors\n","Actors and actresses are those who perform on stage, television, or in movies, both professionally and as amateurs.\n","Adults\n","Adults are characters that are between the ages of 20 to 64.\n","Afros\n","These characters sport an Afro, a large, rounded hairstyle achieved by combing hair outwards from the scalp with a pick.\n","Agents of the Afterlife\n","Afterlife\n","Angels\n","Demons\n","Shinigami\n","These characters perform a job for their employers in the Afterlife, whether they're sent from Heaven, Hell, Purgatory, or somewhere else. Angels, Demons, and Shinigami are common examples.\n","Airheads\n","Ever seen a character and wondered just how they could be THAT dumb? Airheads are individuals who are especially idiotic, dim-witted, or just plain clueless. They often have a lack of common sense, live in their own little worlds or have a childlike dependence on others.\n","Alchemists\n","Alchemists pursue the knowledge and materials for creating such items as the legendary Philosopher's stone and elixir of immortality. They are particularly known for being able to transmute one element into another, as well creating the artificial beings known as the Homunculi.\n","Aliens\n","Aliens hail from a different planet than where the series takes place. For example, if an anime takes place on Mars, a lone Earthling would be considered an alien, not the Martian.\n","Amnesia\n","Illness\n","Supernatural\n","Characters with amnesia have partial or full memory loss. These characters suffer from amnesia due to an injury or trauma, an Illness, or a Supernatural occurrence.\n","Analytical\n","These characters frequently analyze people, objects or situations for the sake of concocting a strategy, whether they're trying to solve a crime, determining how to win a military battle, or figuring out the best way to win a game.\n","Androids\n","Robots\n","Cyborgs\n","Androids are Robots that are made to look and sometimes act as humans, unlike Cyborgs, which are organic characters with robotic parts or upgrades.\n","Androphobia\n","Androphobia is defined as an irrational fear of men. These characters have extreme reactions to the men in their lives, from panicking to having an intense allergic reaction to anything in between.\n","Angels\n","These characters are winged, haloed angels from heaven.\n","Animal Ears\n","These characters have Animal Ears as a natural extension of their body.\n"]}],"source":["import requests\n","from bs4 import BeautifulSoup\n","\n","url = \"https://www.anime-planet.com/characters/tags\"\n","\n","response = requests.get(url)\n","\n","soup = BeautifulSoup(response.content, 'html.parser')\n","\n","tds = soup.findAll('td', {'class': 'volsChRating'})\n","\n","for td in tds:\n","    a_tags = td.findAll('a')\n","    for tag in a_tags:\n","        print(tag.text)\n","        \n","    p_desc=td.findAll('p')\n","    for desc in p_desc:\n","        print(desc.text)"]},{"cell_type":"code","execution_count":null,"id":"8c46201b","metadata":{"papermill":{"duration":0.005844,"end_time":"2023-03-18T19:21:17.776362","exception":false,"start_time":"2023-03-18T19:21:17.770518","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":4,"id":"03f9e4e4","metadata":{"execution":{"iopub.execute_input":"2023-03-18T19:21:17.791312Z","iopub.status.busy":"2023-03-18T19:21:17.790711Z","iopub.status.idle":"2023-03-18T19:21:34.902607Z","shell.execute_reply":"2023-03-18T19:21:34.901395Z"},"papermill":{"duration":17.123438,"end_time":"2023-03-18T19:21:34.905909","exception":false,"start_time":"2023-03-18T19:21:17.782471","status":"completed"},"tags":[]},"outputs":[],"source":["\n","from bs4 import BeautifulSoup\n","import requests\n","import csv\n","url = 'https://www.anime-planet.com/characters/tags'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='w', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=2'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=3'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=4'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=5'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        writer.writerow([a.text,p.text]) \n","url = 'https://www.anime-planet.com/characters/tags?page=6'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=7'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=8'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        if(p is not None):\n","            writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=9'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        if(p is not None):\n","             writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=10'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        if(p is not None):\n","             writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=11'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=12'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=13'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=14'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=15'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        if(p is not None):\n","            writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=16'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=17'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=18'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=19'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=20'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=21'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        if(p is not None):\n","            writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=22'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=23'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        writer.writerow([a.text,p.text]) \n","url = 'https://www.anime-planet.com/characters/tags?page=24'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        if(p is not None):\n","            writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=25'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        if(p is not None):\n","             writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=26'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=27'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=28'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        if(p is not None):\n","            writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=29'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=30'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        if(p is not None):\n","            writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=31'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        if(p is not None):\n","            writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=32'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        if(p is not None):\n","            writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=33'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        if(p is not None):\n","            writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=34'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        if(p is not None):\n","            writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=35'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        if(p is not None):\n","            writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=36'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        if(p is not None):\n","            writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=37'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        if(p is not None):\n","            writer.writerow([a.text,p.text])\n","url = 'https://www.anime-planet.com/characters/tags?page=38'\n","\n","r = requests.get(url)\n","\n","soup = BeautifulSoup(r.text, 'html.parser')\n","\n","with open('output.csv', mode='a', newline='') as file:\n","    for td in soup.find_all('td', class_=\"volsChRating\"):\n","        a = td.find('a')\n","        p = td.find('p')\n","        writer = csv.writer(file)\n","        writer.writerow([a.text,p.text])"]},{"cell_type":"code","execution_count":null,"id":"a6435484","metadata":{"execution":{"iopub.execute_input":"2023-03-18T19:00:22.079998Z","iopub.status.busy":"2023-03-18T19:00:22.078844Z","iopub.status.idle":"2023-03-18T19:00:22.265443Z","shell.execute_reply":"2023-03-18T19:00:22.264154Z","shell.execute_reply.started":"2023-03-18T19:00:22.07995Z"},"papermill":{"duration":0.006179,"end_time":"2023-03-18T19:21:34.918794","exception":false,"start_time":"2023-03-18T19:21:34.912615","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"b4c5660b","metadata":{"papermill":{"duration":0.005886,"end_time":"2023-03-18T19:21:34.930866","exception":false,"start_time":"2023-03-18T19:21:34.92498","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":66.06339,"end_time":"2023-03-18T19:21:35.659876","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-03-18T19:20:29.596486","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}